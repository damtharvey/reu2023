{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why\n",
    "\n",
    "What are you looking at now? Typically in the year 2023, you'd be looking at an a computer display that has about 2 million to 8 million pixels, with 3 color channels each. In the multilayer perceptrons (MLPs) that we've looked at before, taking one of these images as input would require, for example, 3 * 3840 * 2160 = 24883200 parameters in the first layer. This would work, but it's mostly a waste of parameters due to certain properties of natural images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this image in the context of identifying the primary object in the image. \n",
    "<div>\n",
    "<img src=\"data/cat.jpg\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "([Source](https://commons.wikimedia.org/wiki/File:Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that task, this image contains a lot if unnecessary and redundant information. For example, while most of the image is of leaves on the ground, most humans would say that this image is of a cat. Also, we don't really need much information about color or texture to do that. Let's do some image processing to reduce unnecessary information and see whether we can still say it's a cat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I obtained this image as a JPEG file, we can use the file size as a rough proxy for the amount of information in the image. As is, the image size in bytes is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"data/cat.jpg\").stat().st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's change the image to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torchvision.io.read_image(\"data/cat.jpg\")\n",
    "grayscaled_image = image.type(torch.float32).mean(0, keepdim=True).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images: tuple[torch.Tensor, ...] | torch.Tensor):\n",
    "    if not isinstance(images, tuple):\n",
    "        images = (images,)\n",
    "    _, axs = plt.subplots(ncols=len(images), squeeze=False)\n",
    "    for i, image in enumerate(images):\n",
    "        image = image.detach()\n",
    "        if image.shape[0] == 3:\n",
    "            axs[0, i].imshow(image.permute(1, 2, 0).numpy())\n",
    "        else:\n",
    "            axs[0, i].imshow(image[0].numpy(), cmap=\"gray\")\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(grayscaled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how big this would be if saved in JPEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = pathlib.Path(\"temp\")\n",
    "temp_folder.mkdir(exist_ok=True)\n",
    "\n",
    "torchvision.io.write_jpeg(grayscaled_image, str(temp_folder / \"cat_grayscale.jpg\"), quality=90)\n",
    "(temp_folder / \"cat_grayscale.jpg\").stat().st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pretty high quality settings, it's already much smaller.\n",
    "\n",
    "Another thing we could do is lower the number of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscaled_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this resolution is unnecessarily high for the task, we'll downsample it to a quarter the resolution by keeping every fourth pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_image = grayscaled_image[:, ::4, ::4]\n",
    "downsampled_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(downsampled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looks like a cat to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we could do is just keep some edges, say the vertical edges. We can do that by applying a filter to the image using cross-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer, despite its name, is often a **cross-correlation**, followed by an addition.\n",
    "\n",
    "The cross-correlation of vectors $\\mathbf{w}$ and $\\mathbf{x}$ is $\\mathbf{w} \\star \\mathbf{x} = \\mathbf{z} \\in \\mathbb{R}^{|\\mathbf{x}|-|\\mathbf{w}|+1}$ whose each element\n",
    "$\\mathbf{z}_i = \\sum_{j=1}^{|\\mathbf{w}|} \\mathbf{w}_{j} \\mathbf{x}_{i+j-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose $\\mathbf{w} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ and $\\mathbf{x} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}$. Then $\\mathbf{w} \\star \\mathbf{x} = \\begin{bmatrix}1*1 + 2*2 + 3*3 \\\\ 1*2 + 2*3 + 3*4 \\\\ 1*3 + 2*4 + 3*5 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ 20 \\\\ 26 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reverse either one of $\\mathbf{w}$ or $\\mathbf{x}$, you'll get mathematical convolution.\n",
    "> When machine learning people say \"convolution,\" they usually mean cross-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often refer to $\\mathbf{w}$ as the *kernel*, or the *filter*. Some people use *filter* to refer to a set of multiple kernels in the same layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether you follow. Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: the 1-dimensional cross-correlation of w and x\n",
    "    \"\"\"\n",
    "    z = torch.empty(x.shape[0] - w.shape[0] + 1)\n",
    "    for i in range(z.shape[0]):\n",
    "        ... # TODO\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor((1, 2, 3))\n",
    "x = torch.tensor((1, 2, 3, 4, 5))\n",
    "torch.equal(conv1d(x, w), torch.tensor([14, 20, 26]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to see a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'eltpXSA9IHcgQCB4W2kgOiBpICsgdy5zaGFwZVswXV0=').decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply my vertical edge filter, I'll make a kernel like this\n",
    "\n",
    "$\\mathbf{W} = \\begin{bmatrix} -1 & 0 & 1 \\\\\n",
    "                              -1 & 0 & 1 \\\\\n",
    "                              -1 & 0 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and perform a 2-dimensional convolution over the image. You can probably guess how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: the 2-dimensional cross-correlation of w and x\n",
    "    \"\"\"\n",
    "    z = torch.empty(tuple(torch.tensor(x.shape) - torch.tensor(w.shape) + 1))\n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(z.shape[1]):\n",
    "            ...  # TODO\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "w = torch.randn(2, 2)\n",
    "torch.equal(torch.nn.functional.conv2d(x[None, :], w[None, None, :])[0], conv2d(x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'eltpLCBqXSA9ICh3ICogeFtpIDogaSArIHcuc2hhcGVbMF0sIGogOiBqICsgdy5zaGFwZVsxXV0pLnN1bSgp').decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking that this is very inefficient, and you'd be right. For now, let's just use it and leave the performance tweaking to others.\n",
    "\n",
    "Back to our cat image. Let's apply this filter and look at the vertical edges detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edges = conv2d(downsampled_image[0], torch.tensor(((-1, 0, 1),) * 3))[None, :]\n",
    "show_images(vertical_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's harder now, but I can still tell this is a cat. Let's save it as JPEG and check the file size. Since pixel values were summed while applying this filter, we have to rescale the result in order to save it as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edges_image = vertical_edges\n",
    "vertical_edges_image -= vertical_edges_image.min()\n",
    "vertical_edges_image *= 255 / vertical_edges_image.max()\n",
    "vertical_edges_image = vertical_edges_image.to(torch.uint8)\n",
    "torchvision.io.write_jpeg(vertical_edges_image, str(temp_folder / \"cat_vertical_edges.jpg\"), quality=90)\n",
    "(temp_folder / \"cat_vertical_edges.jpg\").stat().st_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this, I ended up with 65404 bytes, down from the original 3586298, so that's about 1/50 the original size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap,\n",
    "\n",
    "> We (humans) wanted to perform image classification.\n",
    "\n",
    "> We observed that not all pixels are equally relevant.\n",
    "\n",
    "> We tried reducing the amount of irrelevant information in the image by processing it using various methods, one of which was cross-correlation with a kernel created to highlight vertical edges.\n",
    "\n",
    "> We observed a great reduction in information while still being able to correctly classify the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we reduced the information so much, we can use multiple kernels to inform our decision. For example, we could use a horizontal edge detector, like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W} = \\begin{bmatrix} -1 & -1 & -1 \\\\\n",
    "                              0 & 0 & 0 \\\\\n",
    "                              1 & 1 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll have both the vertical and horizontal edges to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images((vertical_edges, \n",
    "             conv2d(downsampled_image[0], torch.tensor(((-1, -1, -1), (0, 0, 0), (1, 1, 1))))[None, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> Why is cross-correlation advantageous over simple linear transformations (as in MLPs) in image-related tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll benefit from using a GPU if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural network layers often have many kernels of different shapes and *strides* (how many pixels to move the kernel over after each $i$ or $j$ step; we've been using a stride of 1 in this notebook). The kernels are learnable, so their elements serve as parameters in the neural network.\n",
    "\n",
    "Each kernel learns to detect a certain feature in the input. If they work well, they'll distill the relevant information from the input. Earlier layers detect simple features such as edges, while later layers detect more sophisticated things such as shapes or faces.\n",
    "\n",
    "\n",
    "In classifiers, the combined outputs of the convolutional layers will inform a final linear layer that performs the classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we made a kernel that detected vertical edges.\n",
    "\n",
    "$\\begin{bmatrix} -1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "Let's try to learn this kernel from a random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn $\\mathbf{W}$ as parameters of a function $\\hat{f}$ that we will train to detect vertical edges, i.e. $\\hat{f}$ is just a 2-dimensional cross-correlation.\n",
    "\n",
    "Instead of using the `conv2d` we wrote before, we'll use the `nn.Conv2d` `Module` in PyTorch. PyTorch `Module`s are functions that keep track of their parameters and gradients. You can compose a bunch of `Module`s together and call a `backward` function at the end, which adds the derivative of the composition with respect to all parameters.\n",
    "\n",
    "This `nn.Conv2d` constructor makes a 2-dimensional cross-correlation function that can take in the result of multiple kernels and apply multiple kernels. Since we're only interested in applying one kernel to a single channel image, we put 1 in the relevant arguments. For 3-channel color images, we would be 3 input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=float\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 3), dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Module` has initialized a random kernel. Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d.weight[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a loss function, we'll use the mean squared error (MSE), which is also available as a `Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a training dataset, we'll use MNIST, which is a bunch of images of handwritten digits. Although this dataset is usually used for image classification, we're only using it here to train this one kernel. The training set contains 60000 grayscale images shaped 28 $\\times$ 28.\n",
    "\n",
    "The first time you run the following cell, the MNIST dataset will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_set = torchvision.datasets.MNIST(root=\"data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Conv2d` module expects inputs of shape (batch size, number of channels, height, width). Since MNIST images are single-channel grayscale, we have to insert an extra dimension of size 1 to be the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images = mnist_train_set.data.to(dtype).to(device).unsqueeze(1)\n",
    "mnist_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to avoid blowups, let's scale the pixel values to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our own \"labels\" for this dataset by applying the cross-correlation with the reference kernel. To speed things up, I'll use the `nn.functional` version of `conv2d`, which can handle a batch of inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transformed_images = nn.functional.conv2d(\n",
    "    mnist_images,\n",
    "    torch.tensor(((-1, -1, -1), (0, 0, 0), (1, 1, 1)), dtype=dtype, device=device)[None, None, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example. Below from left to right is\n",
    "* an image in MNIST\n",
    "* what cross-correlation with the randomly initialized $\\mathbf{W}$ does to it\n",
    "* what the reference kernel would do to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    show_images((mnist_images[0].cpu(), \n",
    "                 conv2d(mnist_images[0][None, :])[0].cpu(), \n",
    "                 target_transformed_images[0].cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set some hyperparameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and make a `DataLoader` out of the original images and the desired processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(data.TensorDataset(mnist_images, target_transformed_images),\n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has many `Optimizer`s, which take care of updating parameters and remembering updates to allow more sophisticated update schedules. We'll use `optim.SGD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(conv2d.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the training loop. Upon loading each batch, \n",
    "1. The optimizer zeros all the gradients.\n",
    "1. The $\\hat{f}$ performs cross-correlation on the batch of images using the current kernel.\n",
    "1. The loss function calculates a scalar loss value.\n",
    "1. We call `backward` on the loss value, which adds the gradient of the loss with respect to each parameter to the gradients.\n",
    "1. We call `step` on the `Optimizer`, which updates the parameters using the gradients and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch_number in (progress_bar := tqdm.tqdm(range(N_EPOCHS), desc=f\"training\")):\n",
    "        epoch_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            z = conv2d(x.to(device))\n",
    "            loss = criterion(z, y.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        progress_bar.set_postfix_str(f\"loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've trained for a while. Guess what the kernel is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d.weight[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably not very close to the reference kernel. But let's take a look at a cross-correlation using this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    show_images((mnist_images[0].cpu(), \n",
    "                 conv2d(mnist_images[0][None, :])[0].cpu(), \n",
    "                 target_transformed_images[0].cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, even though the kernel wasn't close, the effect of using it was pretty close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> Why was the learned kernel not close to the reference kernel used to generate the target data, while the effect of using it was similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'Y29udjJkLndlaWdodFswXVswXS5tZWFuKDEp').decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
