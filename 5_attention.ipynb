{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook used material from the PyTorch tutorial [Language Translation with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/translation_transformer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Black to format the code in this notebook. If you want to contribute, use the following to format code cells upon running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T23:52:07.706410800Z",
     "start_time": "2023-05-27T23:52:05.367607800Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install jupyter-black\n",
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colaboratory, go to ```Edit / Notebook settings``` and set the hardware accelerator to ```GPU``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to packages you've installed in previous notebooks, you'll need the natural language processing tools `tiktoken` and `datasets` from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T23:51:39.473113700Z",
     "start_time": "2023-05-27T23:51:31.113431800Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import copy\n",
    "import time\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import tqdm\n",
    "import tiktoken\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use a GPU to run this notebook. If you don't, you might die before completing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if (have_cuda := torch.cuda.is_available()) else \"cpu\")\n",
    "print(f\"\"\"You {\"don't \" * (not have_cuda)}have a GPU.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanisms have become a crucial component in modern neural network architectures, particularly in the field of natural language processing (NLP). The primary motivation behind attention is to allow the model to focus on different parts of the input sequence when making predictions, rather than treating all parts of the input equally. This is especially important for tasks like machine translation, where the relevance of each word in the input sentence can vary depending on the context.\n",
    "\n",
    "Consider the task of translating a sentence from one language to another. Traditional sequence-to-sequence models, such as those using recurrent neural networks (RNNs), process the input sequence in a fixed order and generate the output sequence one token at a time. However, these models often struggle with long sentences because they need to compress all the information from the input sequence into a fixed-size context vector. This can lead to information loss and poor performance on long sentences.\n",
    "\n",
    "Attention mechanisms address this issue by allowing the model to dynamically focus on different parts of the input sequence at each step of the output generation. Instead of relying on a single context vector, the model computes a weighted sum of the input representations, where the weights are determined by the relevance of each input token to the current output token being generated. This allows the model to capture long-range dependencies and handle long sentences more effectively.\n",
    "\n",
    "In the context of the Transformer architecture, attention mechanisms are used extensively in both the encoder and decoder layers. The encoder uses self-attention to capture relationships between tokens in the input sequence, while the decoder uses self-attention to capture relationships within the output sequence and cross-attention to capture relationships between the input and output sequences.\n",
    "\n",
    "The following sections will guide you through the implementation of a Transformer model with attention mechanisms, and demonstrate how it can be used for tasks like machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have been a thing for quite a while now, and many high-quality educational materials have been developed for it. The following materials are arranged roughly in order of detail from vague to specific.\n",
    "* [3Blue1Brown: Large Language Models explained briefly](https://youtu.be/LPZh9BOjkQs?si=9hE7YQ1TMp8_obOz)\n",
    "* [3Blue1Brown: Attention in Transformers, visually explained](https://youtu.be/eMlx5fFNoYc?si=cAo42oOGAKAhOUZW)\n",
    "* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "It's a hot topic, so new materials are always coming out. Search around, or have a chat with the assistant of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll implement masked multi-head attention, and stick it into a Transformer, diagrammed in the following figure.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "(Transformer diagram from [Attention Is All You Need](https://commons.wikimedia.org/wiki/File:Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg). The Transformer is composed of an encoder shown on the left half and a decoder shown on the right half.)\n",
    "\n",
    "We will then use the Transformer to translate German to English using the [Multi30k](https://www.statmt.org/wmt16/multimodal-task.html#task1) dataset. This dataset consists of English and German descriptions of images, among other things, but we'll use only the English and German sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create the input embedding and output embedding layers.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_embedding.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "They take integer token indices and learn vector representations of them. They are just `nn.Embedding` modules with the output scaled by the square root of the number of elements in each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Number of tokens in the token space.\n",
    "        :param embed_dim: Number of elements in embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param tokens: Tensor of integers.\n",
    "        :returns: Embeddings of tokens, shaped (*(tokens shape), embedding size)\n",
    "        \"\"\"\n",
    "        return self.embedding(tokens) * self.embed_dim**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> Why scale it by the square root of the embedding size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the positional encoding layers.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_positional.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ancient times, when recurrent neural networks were typically used for language translation, we didn't need to encode the positions of input tokens, since they were processed in order (front-to-back and back-to-front). We don't want to do that now because it's slow, but we still need some way to marking where in a sequence an input token occurred. There are a few ways of doing this, but the Transformer paper interleaved some scaled sine and cosine values, to be added to the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float, max_length: int = 5000):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Number of elements in embedding vectors.\n",
    "        :param dropout: Probability of zeroing an output element.\n",
    "        :param max_length: Longest sequence length supported.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ln_10000 = 9.21034049987793\n",
    "        positional_encoding = torch.zeros((max_length, embed_dim))\n",
    "        positions = torch.arange(0, max_length).reshape(max_length, 1)\n",
    "        scale = torch.exp(-torch.arange(0, embed_dim, 2) * ln_10000 / embed_dim)\n",
    "        positional_encoding[:, 0::2] = torch.sin(positions * scale)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positions * scale)\n",
    "        positional_encoding = positional_encoding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param token_embedding: Token embeddings shaped (sequence length, batch size, embedding size)\n",
    "        :returns: Embeddings with positional encoding added, possibly with some dropouts.\n",
    "        \"\"\"\n",
    "        return self.dropout(token_embedding + self.positional_encoding[: token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose there were embeddings of size 64, and a sequence of length 100. The following positional encodings would be added to the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(PositionalEncoding(64, 0, 100).positional_encoding[:, 0, :].permute(1, 0).numpy(), cmap=\"coolwarm\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Positional Encoding\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"encoding element\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> For encoding position, why not just use an incrementing integer or a real number that goes from 0 to 1? By the way, the most common appoach today is to use an embedding layer to encode position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: Implement (masked) multi-head attention.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_attention.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Masked multi-head attention can be turned into (unmasked) multi-head attention by using it without a mask. A mask is applied in the decoder attention module to prevent attending to future tokens when predicting the current token. These attention modules sit inside an encoder or decoder layer, and such layers are composed multiple times to make an encoder or a decoder.\n",
    "\n",
    "Inside a multi-head attention module we do the following. Look for references to these steps in the comments to help you implement it. There are things you have to write in both the constructor `__init__` and in the `forward` method.\n",
    "1. We take three copies of the input embeddings (called the *query*, *key*, and *value*), and do a linear transformation on each to a typically smaller size. This is sometimes called *in-projection*. To do this, instantiate three `nn.Linear` modules in the constructor `__init__`. Then, in the `forward` method, call those instances.\n",
    "\n",
    "1. We calculate the scaled dot product attention weights through a batched (broadcasted) matrix multiplication. The broadcast will be over all but the last two dimensions of the operands. Thus, before performing matrix multiplication, we reshape the projected query, key, and value from shape (sequence length, batch size, embedding size) to (batch size, number of heads, sequence length, head size), where head size is the embedding size divided by the number of heads. PyTorch provides some reshaping functions including `view`, `reshape`, `transpose`, and `permute`. See [this blog post](https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/) for a comparison of them. After appropriately reshaping, calculate attention weights as $\\frac{Q K^T}{\\sqrt{h}}$, where $Q$ is the projected query, $K$ is the projected key, and $h$ is the head size.\n",
    "\n",
    "1. If a mask is provided, we add it to the weights. Remember that mask elements are either 0 or $-\\infty$.\n",
    "\n",
    "1. We apply softmax and dropout to the attention weights. If you prefer, you can instantiate `nn.Softmax` and `nn.Dropout` modules to do this.\n",
    "\n",
    "1. We matrix multiply the attention weights with the projected value.\n",
    "\n",
    "1. We concatenate all the head outputs and apply a linear transformation, called *out-projection*. At this point, the attended output is shaped (batch size, number of heads, target sequence length, head size). We should concatenate the head outputs, and reshape the batch and sequence axes into the same axis. The result should be shaped (batch size $*$ target sequence length, embedding size). Perform an out-projection on this using an `nn.Linear` module that you instantiate in the constructor. Then, reshape the output in preparation for the next layer, which expects a shape (target sequence length, batch size, embedding size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0, kdim: int = None, vdim: int = None):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Size of embeddings, must be a multiple of num_heads.\n",
    "        :param num_heads: Number of parallel attention heads. Each head attends to embed_dim / num_heads elements.\n",
    "        :param dropout: Dropout probability on attention weights.\n",
    "        :param kdim: Total number of features for keys. If None, kdim=embed_dim.\n",
    "        :param vdim: Total number of features for values. If None, vdim=embed_dim.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_probability = dropout\n",
    "\n",
    "        # STEP 1: IN-PROJECTION\n",
    "        self.query_linear = ...  # TODO\n",
    "        self.key_linear = ...  # TODO\n",
    "        self.value_linear = ...  # TODO\n",
    "        # END STEP 1\n",
    "\n",
    "        # STEP 6: OUT-PROJECTION\n",
    "        self.out_projection = ...  # TODO\n",
    "        # END STEP 6\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        key_padding_mask: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :param query: query embeddings shaped (target sequence length, batch size, embedding size)\n",
    "        :param key: key embeddings shaped (source sequence length, batch size, embedding size)\n",
    "        :param value: value embeddings shaped (source sequence length, batch size, embedding size)\n",
    "        :param key_padding_mask: floating-point mask shaped (batch size, source sequence length).\n",
    "            This mask will be added to the attention mask before softmax.\n",
    "            To ignore elements in the key embedding, put -inf in corresponding places in the mask.\n",
    "        :param attention_mask: floating-point mask shaped (target sequence length, source sequence length).\n",
    "            This mask will be added to the attention weights before softmax.\n",
    "        :returns: tuple of (attention output, attention weight).\n",
    "            Attention output is shaped (target sequence length, batch size, embedding size).\n",
    "            Attention weight is shaped (batch_size, number of heads, target sequence length, source sequence length).\n",
    "\n",
    "        Simplification of nn.functional.multi_head_attention_forward\n",
    "        \"\"\"\n",
    "\n",
    "        target_length, batch_size, _ = query.shape\n",
    "        source_length, _, _ = key.shape\n",
    "\n",
    "        # STEP 1: In-projection\n",
    "        projected_query = ...  # TODO\n",
    "        projected_key = ...  # TODO\n",
    "        projected_value = ...  # TODO\n",
    "        # END STEP 1\n",
    "\n",
    "        # STEP 2: Calculate attention weights\n",
    "        # Need to reshape to prepare for batched matrix multiplication.\n",
    "        # Reshape from (sequence length, batch size, embedding size)\n",
    "        # to (batch size, number of heads, sequence length, head size)\n",
    "        projected_query = ...  # TODO\n",
    "        projected_key = ...  # TODO\n",
    "        projected_value = ...  # TODO\n",
    "        # Calculate scaled attention. Do not softmax yet.\n",
    "        attention_weight = ...  # TODO\n",
    "        # END STEP 2\n",
    "\n",
    "        # STEP 3: Apply masks\n",
    "        mask = self._combine_masks(key_padding_mask, attention_mask, target_length, batch_size, source_length)\n",
    "        # The combined mask is shaped (batch_size, number of heads, target sequence length, source sequence length).\n",
    "        if mask is not None:\n",
    "            ...  # TODO\n",
    "        # END STEP 3\n",
    "\n",
    "        # STEP 4: Softmax and dropout\n",
    "        attention_weight = ...  # TODO\n",
    "        # END STEP 4\n",
    "\n",
    "        # STEP 5\n",
    "        attended_output = ...  # TODO\n",
    "        # END STEP 5\n",
    "\n",
    "        # STEP 6\n",
    "        attended_output = ...  # TODO: concatenate and reshape\n",
    "        attended_output = ...  # TODO: out-projection\n",
    "        attended_output = ...  # TODO: reshape\n",
    "        # END STEP 6\n",
    "\n",
    "        return attended_output, attention_weight\n",
    "\n",
    "    def _combine_masks(\n",
    "        self,\n",
    "        key_padding_mask: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        target_length: int,\n",
    "        batch_size: int,\n",
    "        source_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Combine and reshape the key padding mask and attention mask.\n",
    "        \"\"\"\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = (\n",
    "                key_padding_mask.view(batch_size, 1, 1, source_length)\n",
    "                .expand(-1, self.num_heads, -1, -1)\n",
    "                .reshape(batch_size * self.num_heads, 1, source_length)\n",
    "            )\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask + key_padding_mask  # Don't say +=.\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.shape[0] == 1:  # batch size is 1 and there is 1 head\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "            else:\n",
    "                attention_mask = attention_mask.view(batch_size, self.num_heads, target_length, source_length)\n",
    "        return attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following code cells to see a solution. Since there are many ways to do this, it's quite likely that these differ from your implementation, while the result is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     base64.b64decode(\n",
    "#         b\"SW4gX19pbml0X186CnNlbGYucXVlcnlfbGluZWFyID0gbm4uTGluZWFyKHNlbGYuZW1iZWRfZGltLCBzZWxmLmVtYmVkX2RpbSkKc2VsZi5rZXlfbGluZWFyID0gbm4uTGluZWFyKHNlbGYua2RpbSwgc2VsZi5lbWJlZF9kaW0pCnNlbGYudmFsdWVfbGluZWFyID0gbm4uTGluZWFyKHNlbGYudmRpbSwgc2VsZi5lbWJlZF9kaW0pCgpJbiBmb3J3YXJkOgpwcm9qZWN0ZWRfcXVlcnkgPSBzZWxmLnF1ZXJ5X2xpbmVhcihxdWVyeSkKcHJvamVjdGVkX2tleSA9IHNlbGYua2V5X2xpbmVhcihrZXkpCnByb2plY3RlZF92YWx1ZSA9IHNlbGYudmFsdWVfbGluZWFyKHZhbHVlKQ==\"\n",
    "#     ).decode()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     base64.b64decode(\n",
    "#         b\"cHJvamVjdGVkX3F1ZXJ5ID0gKAogICAgcHJvamVjdGVkX3F1ZXJ5LnZpZXcodGFyZ2V0X2xlbmd0aCwgYmF0Y2hfc2l6ZSAqIHNlbGYubnVtX2hlYWRzLCBzZWxmLmhlYWRfZGltKQogICAgLnRyYW5zcG9zZSgwLCAxKQogICAgLnZpZXcoYmF0Y2hfc2l6ZSwgc2VsZi5udW1faGVhZHMsIHRhcmdldF9sZW5ndGgsIHNlbGYuaGVhZF9kaW0pCikKcHJvamVjdGVkX2tleSA9ICgKICAgIHByb2plY3RlZF9rZXkudmlldyhwcm9qZWN0ZWRfa2V5LnNoYXBlWzBdLCBiYXRjaF9zaXplICogc2VsZi5udW1faGVhZHMsIHNlbGYuaGVhZF9kaW0pCiAgICAudHJhbnNwb3NlKDAsIDEpCiAgICAudmlldyhiYXRjaF9zaXplLCBzZWxmLm51bV9oZWFkcywgc291cmNlX2xlbmd0aCwgc2VsZi5oZWFkX2RpbSkKKQpwcm9qZWN0ZWRfdmFsdWUgPSAoCiAgICBwcm9qZWN0ZWRfdmFsdWUudmlldyhwcm9qZWN0ZWRfdmFsdWUuc2hhcGVbMF0sIGJhdGNoX3NpemUgKiBzZWxmLm51bV9oZWFkcywgc2VsZi5oZWFkX2RpbSkKICAgIC50cmFuc3Bvc2UoMCwgMSkKICAgIC52aWV3KGJhdGNoX3NpemUsIHNlbGYubnVtX2hlYWRzLCBzb3VyY2VfbGVuZ3RoLCBzZWxmLmhlYWRfZGltKQopCgphdHRlbnRpb25fd2VpZ2h0ID0gcHJvamVjdGVkX3F1ZXJ5IEAgcHJvamVjdGVkX2tleS50cmFuc3Bvc2UoLTIsIC0xKSAvIHByb2plY3RlZF9xdWVyeS5zaXplKC0xKSAqKiAwLjU=\"\n",
    "#     ).decode()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b\"YXR0ZW50aW9uX3dlaWdodCArPSBtYXNr\").decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     base64.b64decode(\n",
    "#         b\"YXR0ZW50aW9uX3dlaWdodCA9IG5uLmZ1bmN0aW9uYWwuZHJvcG91dCgKICAgIHRvcmNoLnNvZnRtYXgoYXR0ZW50aW9uX3dlaWdodCwgZGltPS0xKSwgc2VsZi5kcm9wb3V0X3Byb2JhYmlsaXR5LCBzZWxmLnRyYWluaW5nCik=\"\n",
    "#     ).decode()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b\"YXR0ZW5kZWRfb3V0cHV0ID0gYXR0ZW50aW9uX3dlaWdodCBAIHByb2plY3RlZF92YWx1ZQ==\").decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     base64.b64decode(\n",
    "#         b\"SW4gX19pbml0X186CnNlbGYub3V0X3Byb2plY3Rpb24gPSBubi5MaW5lYXIoc2VsZi5lbWJlZF9kaW0sIHNlbGYuZW1iZWRfZGltKQoKSW4gZm9yd2FyZDoKYXR0ZW5kZWRfb3V0cHV0ID0gYXR0ZW5kZWRfb3V0cHV0LnBlcm11dGUoMiwgMCwgMSwgMykuY29udGlndW91cygpLnZpZXcoYmF0Y2hfc2l6ZSAqIHRhcmdldF9sZW5ndGgsIHNlbGYuZW1iZWRfZGltKQphdHRlbmRlZF9vdXRwdXQgPSBzZWxmLm91dF9wcm9qZWN0aW9uKGF0dGVuZGVkX291dHB1dCkKYXR0ZW5kZWRfb3V0cHV0ID0gYXR0ZW5kZWRfb3V0cHV0LnZpZXcodGFyZ2V0X2xlbmd0aCwgYmF0Y2hfc2l6ZSwgYXR0ZW5kZWRfb3V0cHV0LnNpemUoMSkpCg==\"\n",
    "#     ).decode()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the following classes combine multi-head attention, layer norm, and linear layers into an encoder layer, and compose a bunch of these into an encoder.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_encoder.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: callable = torch.nn.functional.relu,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: the number of expected features in the input.\n",
    "        :param nhead: the number of heads in the multiheadattention models.\n",
    "        :param dim_feedforward: the dimension of the feedforward network model.\n",
    "        :param dropout: the dropout value (default=0.1).\n",
    "        :param activation: the activation function of the intermediate layer\n",
    "        :param layer_norm_eps: the eps value in layer normalization components.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self, source: torch.Tensor, source_mask: torch.Tensor = None, source_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input through the encoder layer.\n",
    "\n",
    "        :param src: the sequence to the encoder layer.\n",
    "        :param source_mask: the mask for the src sequence.\n",
    "        :param source_key_padding_mask: the mask for the src keys per batch.\n",
    "        \"\"\"\n",
    "        x = source\n",
    "        x = self.norm1(x + self._self_attention_block(x, source_mask, source_key_padding_mask))\n",
    "        x = self.norm2(x + self._feedforward_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _self_attention_block(\n",
    "        self, x: torch.Tensor, attention_mask: torch.Tensor, key_padding_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        x, _ = self.self_attention(x, x, x, attention_mask=attention_mask, key_padding_mask=key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _feedforward_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer: nn.Module, num_layers: int, norm: nn.Module = None):\n",
    "        \"\"\"\n",
    "        TransformerEncoder is a stack of encoder layers. Users can build the\n",
    "        BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.\n",
    "\n",
    "        :param encoder_layer: an instance of the TransformerEncoderLayer class.\n",
    "        :param num_layers: the number of sub-encoder-layers in the encoder.\n",
    "        :param norm: the layer normalization component.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, mask: torch.Tensor = None, source_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input through the encoder layers in turn.\n",
    "        :param src: the sequence to the encoder.\n",
    "        :param mask: the mask for the src sequence.\n",
    "        :param source_key_padding_mask: the mask for the src keys per batch.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, source_mask=mask, source_key_padding_mask=source_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the encoder, we make a decoder layer and compose a bunch of decoder layers into a decoder.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_decoder.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: callable = torch.nn.functional.relu,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: the number of expected features in the input.\n",
    "        :param nhead: the number of heads in the multiheadattention models.\n",
    "        :param dim_feedforward: the dimension of the feedforward network model.\n",
    "        :param dropout: the dropout probability.\n",
    "        :param activation: the activation function of the intermediate layer.\n",
    "        :param layer_norm_eps: the eps value in layer normalization components.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attention = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.saved_attention_weight = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        target_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        target_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "        :param target: the sequence to the decoder layer.\n",
    "        :param memory: the sequence from the last layer of the encoder.\n",
    "        :param target_mask: the mask for the target sequence.\n",
    "        :param memory_mask: the mask for the memory sequence.\n",
    "        :param target_key_padding_mask: the mask for the target keys per batch.\n",
    "        :param memory_key_padding_mask: the mask for the memory keys per batch.\n",
    "        \"\"\"\n",
    "        x = target\n",
    "\n",
    "        x = self.norm1(x + self._self_attention_block(x, target_mask, target_key_padding_mask))\n",
    "        x = self.norm2(x + self._multihead_attention_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "        x = self.norm3(x + self._feedforward_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _self_attention_block(self, x, attention_mask, key_padding_mask) -> torch.Tensor:\n",
    "        x, _ = self.self_attention(x, x, x, attention_mask=attention_mask, key_padding_mask=key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _multihead_attention_block(self, x, mem, attention_mask, key_padding_mask) -> torch.Tensor:\n",
    "        x, self.saved_attention_weight = self.multihead_attention(\n",
    "            x, mem, mem, attention_mask=attention_mask, key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        return self.dropout2(x)\n",
    "\n",
    "    def _feedforward_block(self, x) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer: nn.Module, num_layers: int, norm: nn.Module = None):\n",
    "        \"\"\"\n",
    "        TransformerDecoder is a stack of decoder layers\n",
    "\n",
    "        :param decoder_layer: an instance of the TransformerDecoderLayer class.\n",
    "        :param num_layers: the number of sub-decoder-layers in the decoder.\n",
    "        :param norm: the layer normalization component.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target,\n",
    "        memory,\n",
    "        target_mask=None,\n",
    "        memory_mask=None,\n",
    "        target_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        :param target: the sequence to the decoder.\n",
    "        :param memory: the sequence from the last layer of the encoder.\n",
    "        :param target_mask: the mask for the target sequence.\n",
    "        :param memory_mask: the mask for the memory sequence.\n",
    "        :param target_key_padding_mask: the mask for the target keys per batch.\n",
    "        :param memory_key_padding_mask: the mask for the memory keys per batch.\n",
    "        \"\"\"\n",
    "        output = target\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(\n",
    "                output,\n",
    "                memory,\n",
    "                target_mask=target_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                target_key_padding_mask=target_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "            )\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composition of the encoder and the decoder will form our `Transformer` module.\n",
    "<div>\n",
    "<img src=\"data/transformer_torch.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: callable = torch.nn.functional.relu,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: the number of expected features in the encoder or decoder inputs.\n",
    "        :param nhead: the number of heads in the multiheadattention models.\n",
    "        :param num_encoder_layers: the number of sub-encoder-layers in the encoder.\n",
    "        :param num_decoder_layers: the number of sub-decoder-layers in the decoder.\n",
    "        :param dim_feedforward: the dimension of the feedforward network model.\n",
    "        :param dropout: the dropout probability.\n",
    "        :param activation: the activation function of encoder and decoder intermediate layer\n",
    "        :param layer_norm_eps: the eps value in layer normalization components.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n",
    "        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n",
    "        decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        for parameter in self.parameters():\n",
    "            if parameter.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(parameter)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        source_mask: torch.Tensor = None,\n",
    "        target_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        source_key_padding_mask: torch.Tensor = None,\n",
    "        target_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Take in and process masked source/target sequences.\n",
    "        :param src: the sequence to the encoder.\n",
    "        :param target: the sequence to the decoder.\n",
    "        :param source_mask: the additive mask for the src sequence.\n",
    "        :param target_mask: the additive mask for the target sequence.\n",
    "        :param memory_mask: the additive mask for the encoder output.\n",
    "        :param source_key_padding_mask: the Tensor mask for src keys per batch.\n",
    "        :param target_key_padding_mask: the Tensor mask for target keys per batch.\n",
    "        :param memory_key_padding_mask: the Tensor mask for memory keys per batch.\n",
    "        \"\"\"\n",
    "        memory = self.encoder(src, mask=source_mask, source_key_padding_mask=source_key_padding_mask)\n",
    "        output = self.decoder(\n",
    "            target,\n",
    "            memory,\n",
    "            target_mask=target_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            target_key_padding_mask=target_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int, device=\"cpu\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        return torch.triu(torch.full((sz, sz), float(\"-inf\"), device=device), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice a few differences between this `Transformer` module and the Transformer diagram we've been looking at.\n",
    "* This module doesn't include the embedding and positional encoding layers.\n",
    "* This module doesn't include the final linear and softmax layers.\n",
    "\n",
    "We've done it this way to make it similar to the `nn.Transformer` module in PyTorch 2.0. If we want to use it for natural language translation, we have to add these layers except softmax to model that we'll train to translate sentences, which is the following `Seq2SeqTransformer`. We don't need the final softmax because we'll just take the argmax of the outputs to get the predicted token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        nhead: int,\n",
    "        source_vocab_size: int,\n",
    "        target_vocab_size: int,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sequence-to-sequence transformer for translating natural languages.\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.source_token_embedding = TokenEmbedding(source_vocab_size, emb_size)\n",
    "        self.target_token_embedding = TokenEmbedding(target_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, target_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        source_mask: torch.Tensor,\n",
    "        target_mask: torch.Tensor,\n",
    "        source_padding_mask: torch.Tensor,\n",
    "        target_padding_mask: torch.Tensor,\n",
    "        memory_key_padding_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.generator(\n",
    "            self.transformer(\n",
    "                self.positional_encoding(self.source_token_embedding(source)),\n",
    "                self.positional_encoding(self.target_token_embedding(target)),\n",
    "                source_mask,\n",
    "                target_mask,\n",
    "                None,\n",
    "                source_padding_mask,\n",
    "                target_padding_mask,\n",
    "                memory_key_padding_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def encode(self, src: torch.Tensor, source_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.transformer.encoder(self.positional_encoding(self.source_token_embedding(src)), source_mask)\n",
    "\n",
    "    def decode(self, target: torch.Tensor, memory: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.transformer.decoder(\n",
    "            self.positional_encoding(self.target_token_embedding(target)), memory, target_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate natural languages, we have to prepare the tokenizers and build the vocabularies (token space) for English and German. We will prepare special beginning-of-sequence (BOS) and end-of-sequence (EOS) to mark sequence boundaries. We also have an unknown (UNK) token to handle subsequences outside the tokenizer's vocabulary, or tokens that appear too infrequently for us to care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = (SOURCE_LANGUAGE := \"de\", TARGET_LANGUAGE := \"en\")\n",
    "SPECIAL_SYMBOLS = (UNK := \"<unk>\", PAD := \"<pad>\", BOS := \"<bos>\", EOS := \"<eos>\")\n",
    "SPECIAL_SYMBOL_INDICES = {symbol: index for index, symbol in enumerate(SPECIAL_SYMBOLS)}\n",
    "MINIMUM_FREQUENCY = 2\n",
    "\n",
    "tokenizer = {\n",
    "    SOURCE_LANGUAGE: tiktoken.get_encoding(\"gpt2\"),\n",
    "    TARGET_LANGUAGE: tiktoken.get_encoding(\"gpt2\"),\n",
    "}\n",
    "\n",
    "\n",
    "def yield_tokens(data_iterable: Iterable, language: str) -> list[str]:\n",
    "    for data_sample in data_iterable:\n",
    "        yield tokenizer[language].encode(data_sample[language])\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-de-en\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "vocab_transform = {}\n",
    "for language in LANGUAGES:\n",
    "    vocab_transform[language] = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some helper functions for messing with natural language datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    \"\"\"\n",
    "    Compose a bunch of transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids: list[int]):\n",
    "    \"\"\"\n",
    "    Add BOS/EOS and create tensor for input sequence indices\n",
    "    \"\"\"\n",
    "    return torch.cat(\n",
    "        (\n",
    "            torch.tensor([SPECIAL_SYMBOL_INDICES[BOS]]),\n",
    "            torch.tensor(token_ids),\n",
    "            torch.tensor([SPECIAL_SYMBOL_INDICES[EOS]]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Source and target language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {\n",
    "    language: sequential_transforms(tokenizer[language].encode, tensor_transform) for language in LANGUAGES\n",
    "}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate data samples into batch tensors\n",
    "    \"\"\"\n",
    "    source_batch, target_batch = [], []\n",
    "    for data_sample in batch:\n",
    "        source_sample, target_sample = (\n",
    "            data_sample[\"translation\"][SOURCE_LANGUAGE],\n",
    "            data_sample[\"translation\"][TARGET_LANGUAGE],\n",
    "        )\n",
    "        source_batch.append(text_transform[SOURCE_LANGUAGE](source_sample.rstrip(\"\\n\")))\n",
    "        target_batch.append(text_transform[TARGET_LANGUAGE](target_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    source_batch = nn.utils.rnn.pad_sequence(source_batch, padding_value=SPECIAL_SYMBOL_INDICES[PAD])\n",
    "    target_batch = nn.utils.rnn.pad_sequence(target_batch, padding_value=SPECIAL_SYMBOL_INDICES[PAD])\n",
    "    return source_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create triangular masks and padding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(source, target):\n",
    "    source_sequence_length = source.shape[0]\n",
    "    target_sequence_length = target.shape[0]\n",
    "\n",
    "    target_mask = Transformer.generate_square_subsequent_mask(target_sequence_length, DEVICE)\n",
    "    source_mask = torch.zeros((source_sequence_length, source_sequence_length), device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    source_padding_mask = torch.zeros_like(source, dtype=torch.float32)\n",
    "    target_padding_mask = torch.zeros_like(target, dtype=torch.float32)\n",
    "    source_padding_mask[source == SPECIAL_SYMBOL_INDICES[PAD]] = -torch.inf\n",
    "    target_padding_mask[target == SPECIAL_SYMBOL_INDICES[PAD]] = -torch.inf\n",
    "    source_padding_mask = source_padding_mask.transpose(0, 1)\n",
    "    target_padding_mask = target_padding_mask.transpose(0, 1)\n",
    "\n",
    "    return source_mask, target_mask, source_padding_mask, target_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define the hyperparameters for our Transformer. If you have difficulty running this notebook, change these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VOCAB_SIZE = tokenizer[SOURCE_LANGUAGE].n_vocab\n",
    "TARGET_VOCAB_SIZE = tokenizer[TARGET_LANGUAGE].n_vocab\n",
    "EMBED_SIZE = 512\n",
    "NUM_HEADS = 8\n",
    "FEEDFORWARD_HIDDEN_SIZE = 512\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make the train and validation data loaders. I happened to know the number of training and validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-de-en\", split=\"validation\", trust_remote_code=True),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model, loss function, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMBED_SIZE,\n",
    "    NUM_HEADS,\n",
    "    SOURCE_VOCAB_SIZE,\n",
    "    TARGET_VOCAB_SIZE,\n",
    "    FEEDFORWARD_HIDDEN_SIZE,\n",
    ").to(DEVICE, dtype=DTYPE)\n",
    "for parameter in model.parameters():\n",
    "    if parameter.dim() > 1:\n",
    "        nn.init.xavier_uniform_(parameter)\n",
    "model = model.to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=SPECIAL_SYMBOL_INDICES[PAD])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for source, target in (progress_bar := tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch}\")):\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        target_input = target[:-1, :]\n",
    "        source_mask, target_mask, source_padding_mask, target_padding_mask = create_masks(source, target_input)\n",
    "        logits = model(\n",
    "            source,\n",
    "            target_input,\n",
    "            source_mask,\n",
    "            target_mask,\n",
    "            source_padding_mask,\n",
    "            target_padding_mask,\n",
    "            source_padding_mask,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target_out = target[1:, :]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total += target.shape[1]\n",
    "\n",
    "        progress_bar.set_postfix_str(f\"train loss: {train_loss / total:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for source, target in (progress_bar := tqdm.tqdm(validation_dataloader, desc=f\"Validation\")):\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        target_input = target[:-1, :]\n",
    "        source_mask, target_mask, source_padding_mask, target_padding_mask = create_masks(source, target_input)\n",
    "        logits = model(\n",
    "            source,\n",
    "            target_input,\n",
    "            source_mask,\n",
    "            target_mask,\n",
    "            source_padding_mask,\n",
    "            target_padding_mask,\n",
    "            source_padding_mask,\n",
    "        )\n",
    "        target_out = target[1:, :]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_out.reshape(-1))\n",
    "        validation_loss += loss.item()\n",
    "        total += target.shape[1]\n",
    "\n",
    "        progress_bar.set_postfix_str(f\"validation loss: {validation_loss / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following `translate` function to try translating a German sentence to English, and optionally plot the attention of each head in the final decoder layer. Note that we trained on **descriptions of images**, so the model will do better on sentences like that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(\n",
    "    model: nn.Module, source: torch.Tensor, source_mask: torch.Tensor, max_length: int, start_symbol: int\n",
    ") -> torch.Tensor:\n",
    "    source = source.to(DEVICE)\n",
    "    memory = model.encode(source, source_mask)\n",
    "    predicted_tokens = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for _ in range(max_length - 1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        target_mask = Transformer.generate_square_subsequent_mask(predicted_tokens.size(0)).to(DEVICE)\n",
    "        out = model.decode(predicted_tokens, memory, target_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_token = torch.max(prob, dim=1)\n",
    "        next_token = next_token.item()\n",
    "\n",
    "        predicted_tokens = torch.cat([predicted_tokens, torch.ones(1, 1).type_as(source.data).fill_(next_token)], dim=0)\n",
    "        if next_token == SPECIAL_SYMBOL_INDICES[EOS]:\n",
    "            break\n",
    "    return predicted_tokens\n",
    "\n",
    "\n",
    "def translate(model: torch.nn.Module, source_sentence: str, plot_attention: bool = False):\n",
    "    model.eval()\n",
    "    source_token_indices = text_transform[SOURCE_LANGUAGE](source_sentence).view(-1, 1)\n",
    "    num_source_tokens = source_token_indices.shape[0]\n",
    "    target_tokens = greedy_decode(\n",
    "        model,\n",
    "        source_token_indices,\n",
    "        source_mask=None,\n",
    "        max_length=num_source_tokens * 2,\n",
    "        start_symbol=SPECIAL_SYMBOL_INDICES[BOS],\n",
    "    ).flatten()\n",
    "    num_target_tokens = target_tokens.shape[0]\n",
    "\n",
    "    if plot_attention:\n",
    "        n_heads = model.transformer.nhead\n",
    "        n_rows = n_heads // 4\n",
    "        n_cols = n_heads // n_rows\n",
    "\n",
    "        fig = plt.figure(figsize=(num_source_tokens, num_target_tokens))\n",
    "\n",
    "        for head in range(n_heads):\n",
    "            ax = fig.add_subplot(n_rows, n_cols, head + 1)\n",
    "            ax.matshow(\n",
    "                model.transformer.decoder.layers[-1].saved_attention_weight.squeeze(0)[head].cpu().detach().numpy(),\n",
    "                cmap=\"gray\",\n",
    "            )\n",
    "            ax.set_xlabel(\"input tokens\")\n",
    "            ax.set_ylabel(\"output tokens\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    target_str = vocab_transform[TARGET_LANGUAGE].decode(list(target_tokens.cpu().numpy()))\n",
    "    # remove <bos> and <eos> tokens\n",
    "    return target_str[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a source sentence from a song I know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sterben werde ich um zu leben.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable translation would be \"I will die in order to live.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, sentence, plot_attention=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
