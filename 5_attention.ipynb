{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook used material from the PyTorch tutorial [Language Translation with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/translation_transformer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Black to format the code in this notebook. If you want to contribute, use the following to format code cells upon running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T23:52:07.706410800Z",
     "start_time": "2023-05-27T23:52:05.367607800Z"
    }
   },
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to packages you've installed in previous notebooks, you'll need the natural language processing tools `torchtext` and `spacy`. Also, to download the dataset we'll be using, you'll need `portalocker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext spacy portalocker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers split a string into symbols. There are tokenizers for different languages and genres. Download the necessary Spacy tokenizers by entering the following into a terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T23:51:39.473113700Z",
     "start_time": "2023-05-27T23:51:31.113431800Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import copy\n",
    "import time\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use a GPU to run this notebook. If you don't, you might die before completing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if (have_cuda := torch.cuda.is_available()) else \"cpu\")\n",
    "print(f\"\"\"You {\"don't \" if not have_cuda else \"\"}have a GPU.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to make up a language, called Nyan, and teach it to you through parallel examples of Nyan and English. Let's go.\n",
    "\n",
    "* nyan nyaan: Look at me. \n",
    "* nyaa nyaan: Look behind you.\n",
    "* nyaa ya nyauu a: There is a human behind you.\n",
    "* nyauu u nyao: Follow the human.\n",
    "\n",
    "Now, translate the following English sentence into Nyan.\n",
    "\n",
    "* Follow me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to see the expected translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'bnlhbiB1IG55YW8=').decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While processing languages, we often split a stream of information into tokens, which we treat as individual symbols. While looking at those Nyan example sentences, you probably treated each sequence of space-separated characters as a token.\n",
    "\n",
    "In the previous examples, I gave one sentence with the idea of *following*, and one with the idea of *me*. I showed at least two examples of the other major concepts *looking*, *behind*, and *human*.\n",
    "\n",
    "Then, when I asked you to translate \"Follow me,\" you probably plated a lot of importance on the tokens *nyan*, *u*, and *nyao*, since you saw those only once, and in those cases, the other tokens had established correlations with other concepts through two examples.\n",
    "\n",
    "In the sentence pair \"nyaa ya nyauu a: There is a human behind you\" which Nyan token corresponds most strongly to the to the English token \"human\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'bnlhdXU=').decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, when presented with the token *human* you're able to focus almost exclusively on this token to learn how to express this concept quickly. If you were to have a correspondence representation in the form of a probability distribution over the tokens [nyaa, ya, nyauu, a], it would be something like $[\\approx 0, \\approx 0, \\approx 1, \\approx 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please read [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/). You should get to know at least the following concepts.\n",
    "* The encoder-decoder structure\n",
    "* Why we want many attention heads\n",
    "* Why we want positional encodings\n",
    "* What masks are for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll implement masked multi-head attention, and stick it into a Transformer, diagrammed in the following figure.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "(Transformer diagram from [Attention Is All You Need](https://commons.wikimedia.org/wiki/File:Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg). The Transformer is composed of an encoder shown on the left half and a decoder shown on the right half.)\n",
    "\n",
    "We will then use the Transformer to translate German to English using the [Multi30k](https://www.statmt.org/wmt16/multimodal-task.html#task1) dataset. This dataset consists of English and German descriptions of images, among other things, but we'll use only the English and German sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create the input embedding and output embedding layers.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_embedding.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "They take integer token indices and learn vector representations of them. They are just `nn.Embedding` modules with the output scaled by the square root of the number of elements in each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Number of tokens in the token space.\n",
    "        :param embed_dim: Number of elements in embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param tokens: Tensor of integers.\n",
    "        :returns: Embeddings of tokens, shaped (*(tokens shape), embedding size)\n",
    "        \"\"\"\n",
    "        return self.embedding(tokens) * self.embed_dim**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> Why scale it by the square root of the embedding size? What happens during attention calculation if we don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the positional encoding layers.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_positional.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ancient times, when recurrent neural networks were typically used for language translation, we didn't need to encode the positions of input tokens, since they were processed in order (front-to-back and back-to-front). We don't want to do that now because it's slow, but we still need some way to marking where in a sequence an input token occurred. There are a few ways of doing this, but the Transformer paper interleaved some scaled sine and cosine values, to be added to the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float, max_length: int = 5000):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Number of elements in embedding vectors.\n",
    "        :param dropout: Probability of zeroing an output element.\n",
    "        :param max_length: Longest sequence length supported.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ln_10000 = 9.21034049987793\n",
    "        positional_encoding = torch.zeros((max_length, embed_dim))\n",
    "        positions = torch.arange(0, max_length).reshape(max_length, 1)\n",
    "        scale = torch.exp(-torch.arange(0, embed_dim, 2) * ln_10000 / embed_dim)\n",
    "        positional_encoding[:, 0::2] = torch.sin(positions * scale)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positions * scale)\n",
    "        positional_encoding = positional_encoding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param token_embedding: Token embeddings shaped (sequence length, batch size, embedding size)\n",
    "        :returns: Embeddings with positional encoding added, possibly with some dropouts.\n",
    "        \"\"\"\n",
    "        return self.dropout(token_embedding + self.positional_encoding[: token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose there were embeddings of size 64, and a sequence of length 100. The following positional encodings would be added to the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(PositionalEncoding(64, 0, 100).positional_encoding[:, 0, :].permute(1, 0).numpy(), cmap=\"coolwarm\")\n",
    "plt.colorbar()\n",
    "plt.title(\"positional encoding\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"encoding element\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll implement (masked) multi-head attention.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_attention.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "All we have to do is implement masked multi-head attention and use it without a mask where appropriate. These attention modules sit inside an encoder or decoder layer, and such layers are composed multiple times to make an encoder or a decoder. A mask is applied in the decoder attention module to prevent attending to future tokens when predicting the current token.\n",
    "\n",
    "Inside a multi-head attention module we do the following. Look for references to these steps in the comments to help you implement it.\n",
    "1. We take three copies of the input embeddings (called the *query*, *key*, and *value*), and do a linear transformation on each to a typically smaller size. This is sometimes called *in-projection*.\n",
    "\n",
    "1. We calculate the attention weights.\n",
    "\n",
    "1. If a mask is provided, we apply it to the weights.\n",
    "\n",
    "1. We apply dropout to the attention weights, at some given dropout probability.\n",
    "\n",
    "1. We multiply the attention weights with the in-projected values.\n",
    "\n",
    "1. We apply a linear transformation, called *out-projection*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: int = 0.0, kdim: int = None, vdim: int = None):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Size of embeddings, must be a multiple of num_heads.\n",
    "        :param num_heads: Number of parallel attention heads. Each head attends to embed_dim / num_heads elements.\n",
    "        :param dropout: Dropout probability on attention weights.\n",
    "        :param kdim: Total number of features for keys. If None, kdim=embed_dim.\n",
    "        :param vdim: Total number of features for values. If None, vdim=embed_dim.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout_probability = dropout\n",
    "\n",
    "        # STEP 1: IN-PROJECTION\n",
    "        self.query_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.key_linear = nn.Linear(self.kdim, self.embed_dim)\n",
    "        self.value_linear = nn.Linear(self.vdim, self.embed_dim)\n",
    "        # END STEP 1\n",
    "\n",
    "        # STEP 6: OUT-PROJECTION\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # END STEP 6\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        key_padding_mask: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :param query: query embeddings shaped (target sequence length, batch size, embedding size)\n",
    "        :param key: key embeddings shaped (source sequence length, batch size, embedding size)\n",
    "        :param value: value embeddings shaped (source sequence length, batch size, embedding size)\n",
    "        :param key_padding_mask: floating-point mask shaped (batch size, source sequence length).\n",
    "            This mask will be added to the attention mask before softmax.\n",
    "            To ignore elements in the key embedding, put -inf in corresponding places in the mask.\n",
    "        :param attention_mask: floating-point mask shaped (target sequence length, source sequence length).\n",
    "            This mask will be added to the attention weights before softmax.\n",
    "        :returns: tuple of (attention output, attention weight).\n",
    "            Attention output is shaped (target sequence length, batch size, embedding size).\n",
    "            Attention weight is shaped ()\n",
    "\n",
    "        Simplification of nn.functional.multi_head_attention_forward\n",
    "        \"\"\"\n",
    "\n",
    "        target_length, batch_size, _ = query.shape\n",
    "        source_length, _, _ = key.shape\n",
    "\n",
    "        # STEP 1: In-projection\n",
    "        q = self.query_linear(query)\n",
    "        k = self.key_linear(key)\n",
    "        v = self.value_linear(value)\n",
    "        # END STEP 1\n",
    "\n",
    "        # Need to reshape to prepare for later linear layers.\n",
    "        # q: (target sequence length, batch size, embedding size)\n",
    "        # -> (batch size * number of heads, target sequence length, head size)\n",
    "        q = q.view(target_length, batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        # k: (source sequence length, batch size, embedding size)\n",
    "        # -> (batch size * number of heads, source sequence length, head size)\n",
    "        k = k.view(k.shape[0], batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        # v: (source sequence length, batch size, embedding size)\n",
    "        # -> (batch size * number of heads, source sequence length, head size)\n",
    "        v = v.view(v.shape[0], batch_size * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        q = q.view(batch_size, self.num_heads, target_length, self.head_dim)\n",
    "        k = k.view(batch_size, self.num_heads, source_length, self.head_dim)\n",
    "        v = v.view(batch_size, self.num_heads, source_length, self.head_dim)\n",
    "\n",
    "        # STEP 2: Calculate attention weights\n",
    "        attention_weight = q @ k.transpose(-2, -1) / q.size(-1) ** 0.5\n",
    "        # END STEP 2\n",
    "\n",
    "        # STEP 3: Apply masks\n",
    "        attention_mask = self._combine_masks(key_padding_mask, attention_mask, target_length, batch_size, source_length)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight += attention_mask\n",
    "        # END STEP 3\n",
    "\n",
    "        # STEP 4: Softmax and dropout\n",
    "        attention_weight = nn.functional.dropout(\n",
    "            torch.softmax(attention_weight, dim=-1), self.dropout_probability, self.training\n",
    "        )\n",
    "        # END STEP 4\n",
    "\n",
    "        # STEP 5\n",
    "        attention_output = attention_weight @ v\n",
    "        # END STEP 5\n",
    "\n",
    "        # STEP 6\n",
    "        attention_output = (\n",
    "            attention_output.permute(2, 0, 1, 3).contiguous().view(batch_size * target_length, self.embed_dim)\n",
    "        )\n",
    "        attention_output = self.out_proj(attention_output)\n",
    "        # END STEP 6\n",
    "\n",
    "        attention_output = attention_output.view(target_length, batch_size, attention_output.size(1))\n",
    "\n",
    "        return attention_output, attention_weight\n",
    "\n",
    "    def _combine_masks(\n",
    "        self,\n",
    "        key_padding_mask: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        target_length: int,\n",
    "        batch_size: int,\n",
    "        source_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = (\n",
    "                key_padding_mask.view(batch_size, 1, 1, source_length)\n",
    "                .expand(-1, self.num_heads, -1, -1)\n",
    "                .reshape(batch_size * self.num_heads, 1, source_length)\n",
    "            )\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask + key_padding_mask  # Don't say +=.\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.shape[0] == 1:  # batch size 1 and 1 head\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "            else:\n",
    "                attention_mask = attention_mask.view(batch_size, self.num_heads, target_length, source_length)\n",
    "        return attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the following classes combine multi-head attention, layer norm, and linear layers into an encoder layer, and compose a bunch of these into an encoder. Note that each layer is the same instance.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_encoder.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: callable = torch.nn.functional.relu,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: the number of expected features in the input.\n",
    "        :param nhead: the number of heads in the multiheadattention models.\n",
    "        :param dim_feedforward: the dimension of the feedforward network model.\n",
    "        :param dropout: the dropout value (default=0.1).\n",
    "        :param activation: the activation function of the intermediate layer\n",
    "        :param layer_norm_eps: the eps value in layer normalization components.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self, source: torch.Tensor, source_mask: torch.Tensor = None, source_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input through the encoder layer.\n",
    "\n",
    "        :param src: the sequence to the encoder layer.\n",
    "        :param source_mask: the mask for the src sequence.\n",
    "        :param source_key_padding_mask: the mask for the src keys per batch.\n",
    "        \"\"\"\n",
    "        x = source\n",
    "        x = self.norm1(x + self._self_attention_block(x, source_mask, source_key_padding_mask))\n",
    "        x = self.norm2(x + self._feedforward_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _self_attention_block(\n",
    "        self, x: torch.Tensor, attention_mask: torch.Tensor, key_padding_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        x, _ = self.self_attention(x, x, x, attention_mask=attention_mask, key_padding_mask=key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _feedforward_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer: nn.Module, num_layers: int, norm: nn.Module = None):\n",
    "        \"\"\"\n",
    "        TransformerEncoder is a stack of encoder layers. Users can build the\n",
    "        BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.\n",
    "\n",
    "        :param encoder_layer: an instance of the TransformerEncoderLayer class.\n",
    "        :param num_layers: the number of sub-encoder-layers in the encoder.\n",
    "        :param norm: the layer normalization component.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, mask: torch.Tensor = None, source_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input through the encoder layers in turn.\n",
    "        :param src: the sequence to the encoder.\n",
    "        :param mask: the mask for the src sequence.\n",
    "        :param source_key_padding_mask: the mask for the src keys per batch.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, source_mask=mask, source_key_padding_mask=source_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the encoder, we make a decoder layer and compose a bunch of decoder layers into a decoder.\n",
    "\n",
    "<div>\n",
    "<img src=\"data/transformer_decoder.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: callable = torch.nn.functional.relu,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: the number of expected features in the input.\n",
    "        :param nhead: the number of heads in the multiheadattention models.\n",
    "        :param dim_feedforward: the dimension of the feedforward network model.\n",
    "        :param dropout: the dropout probability.\n",
    "        :param activation: the activation function of the intermediate layer.\n",
    "        :param layer_norm_eps: the eps value in layer normalization components.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attention = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        target_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        target_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "        :param target: the sequence to the decoder layer.\n",
    "        :param memory: the sequence from the last layer of the encoder.\n",
    "        :param target_mask: the mask for the target sequence.\n",
    "        :param memory_mask: the mask for the memory sequence.\n",
    "        :param target_key_padding_mask: the mask for the target keys per batch.\n",
    "        :param memory_key_padding_mask: the mask for the memory keys per batch.\n",
    "        \"\"\"\n",
    "        x = target\n",
    "\n",
    "        x = self.norm1(x + self._self_attention_block(x, target_mask, target_key_padding_mask))\n",
    "        x = self.norm2(x + self._multihead_attention_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "        x = self.norm3(x + self._feedforward_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _self_attention_block(self, x, attention_mask, key_padding_mask) -> torch.Tensor:\n",
    "        x, _ = self.self_attention(x, x, x, attention_mask=attention_mask, key_padding_mask=key_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _multihead_attention_block(self, x, mem, attention_mask, key_padding_mask) -> torch.Tensor:\n",
    "        x, _ = self.multihead_attention(x, mem, mem, attention_mask=attention_mask, key_padding_mask=key_padding_mask)\n",
    "        return self.dropout2(x)\n",
    "\n",
    "    def _feedforward_block(self, x) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer: nn.Module, num_layers: int, norm: nn.Module = None):\n",
    "        \"\"\"\n",
    "        TransformerDecoder is a stack of decoder layers\n",
    "\n",
    "        :param decoder_layer: an instance of the TransformerDecoderLayer class.\n",
    "        :param num_layers: the number of sub-decoder-layers in the decoder.\n",
    "        :param norm: the layer normalization component.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        target,\n",
    "        memory,\n",
    "        target_mask=None,\n",
    "        memory_mask=None,\n",
    "        target_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        :param target: the sequence to the decoder.\n",
    "        :param memory: the sequence from the last layer of the encoder.\n",
    "        :param target_mask: the mask for the target sequence.\n",
    "        :param memory_mask: the mask for the memory sequence.\n",
    "        :param target_key_padding_mask: the mask for the target keys per batch.\n",
    "        :param memory_key_padding_mask: the mask for the memory keys per batch.\n",
    "        \"\"\"\n",
    "        output = target\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(\n",
    "                output,\n",
    "                memory,\n",
    "                target_mask=target_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                target_key_padding_mask=target_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "            )\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composition of the encoder and the decoder will form the Transformer. This is very similar to the `nn.Transformer` module in PyTorch 2.0.\n",
    "<div>\n",
    "<img src=\"data/transformer_torch.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: callable = torch.nn.functional.relu,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: the number of expected features in the encoder or decoder inputs.\n",
    "        :param nhead: the number of heads in the multiheadattention models.\n",
    "        :param num_encoder_layers: the number of sub-encoder-layers in the encoder.\n",
    "        :param num_decoder_layers: the number of sub-decoder-layers in the decoder.\n",
    "        :param dim_feedforward: the dimension of the feedforward network model.\n",
    "        :param dropout: the dropout probability.\n",
    "        :param activation: the activation function of encoder and decoder intermediate layer\n",
    "        :param layer_norm_eps: the eps value in layer normalization components.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n",
    "        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n",
    "        decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        for parameter in self.parameters():\n",
    "            if parameter.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(parameter)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        source_mask: torch.Tensor = None,\n",
    "        target_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        source_key_padding_mask: torch.Tensor = None,\n",
    "        target_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Take in and process masked source/target sequences.\n",
    "        :param src: the sequence to the encoder.\n",
    "        :param target: the sequence to the decoder.\n",
    "        :param source_mask: the additive mask for the src sequence.\n",
    "        :param target_mask: the additive mask for the target sequence.\n",
    "        :param memory_mask: the additive mask for the encoder output.\n",
    "        :param source_key_padding_mask: the Tensor mask for src keys per batch.\n",
    "        :param target_key_padding_mask: the Tensor mask for target keys per batch.\n",
    "        :param memory_key_padding_mask: the Tensor mask for memory keys per batch.\n",
    "        \"\"\"\n",
    "        memory = self.encoder(src, mask=source_mask, source_key_padding_mask=source_key_padding_mask)\n",
    "        output = self.decoder(\n",
    "            target,\n",
    "            memory,\n",
    "            target_mask=target_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            target_key_padding_mask=target_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int, device=\"cpu\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        return torch.triu(torch.full((sz, sz), float(\"-inf\"), device=device), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice a few differences between this `Transformer` module and the Transformer diagram we've been looking at.\n",
    "* This module doesn't include the embedding and positional encoding layers.\n",
    "* This module doesn't include the final linear and softmax layers.\n",
    "\n",
    "If we want to use it for natural language translation, we have to add these layers except softmax to model that we'll train to translate sentences, which is the following `Seq2SeqTransformer`. We don't need the final softmax because we'll just take the argmax of the outputs to get the predicted token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        nhead: int,\n",
    "        source_vocab_size: int,\n",
    "        target_vocab_size: int,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sequence-to-sequence transformer for translating natural languages.\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.source_token_embedding = TokenEmbedding(source_vocab_size, emb_size)\n",
    "        self.target_token_embedding = TokenEmbedding(target_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, target_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        source_mask: torch.Tensor,\n",
    "        target_mask: torch.Tensor,\n",
    "        source_padding_mask: torch.Tensor,\n",
    "        target_padding_mask: torch.Tensor,\n",
    "        memory_key_padding_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.generator(\n",
    "            self.transformer(\n",
    "                self.positional_encoding(self.source_token_embedding(source)),\n",
    "                self.positional_encoding(self.target_token_embedding(target)),\n",
    "                source_mask,\n",
    "                target_mask,\n",
    "                None,\n",
    "                source_padding_mask,\n",
    "                target_padding_mask,\n",
    "                memory_key_padding_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def encode(self, src: torch.Tensor, source_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.transformer.encoder(self.positional_encoding(self.source_token_embedding(src)), source_mask)\n",
    "\n",
    "    def decode(self, target: torch.Tensor, memory: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.transformer.decoder(\n",
    "            self.positional_encoding(self.target_token_embedding(target)), memory, target_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate natural languages, we have to prepare the tokenizers and build the vocabularies (token space) for English and German. We will prepare special beginning-of-sequence (BOS) and end-of-sequence (EOS) to mark sequnece boundaries. We also have an unknown (UNK) token to handle subsequences outside the tokenizer's vocabulary, or tokens that appear to infrequently for us to care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = (SOURCE_LANGUAGE := \"de\", TARGET_LANGUAGE := \"en\")\n",
    "SPECIAL_SYMBOLS = (UNK := \"<unk>\", PAD := \"<pad>\", BOS := \"<bos>\", EOS := \"<eos>\")\n",
    "SPECIAL_SYMBOL_INDICES = {symbol: index for index, symbol in enumerate(SPECIAL_SYMBOLS)}\n",
    "MINIMUM_FREQUENCY = 2\n",
    "\n",
    "token_transform = {\n",
    "    SOURCE_LANGUAGE: torchtext.data.utils.get_tokenizer(\"spacy\", language=\"de_core_news_sm\"),\n",
    "    TARGET_LANGUAGE: torchtext.data.utils.get_tokenizer(\"spacy\", language=\"en_core_web_sm\"),\n",
    "}\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> list[str]:\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[{SOURCE_LANGUAGE: 0, TARGET_LANGUAGE: 1}[language]])\n",
    "\n",
    "\n",
    "vocab_transform = {}\n",
    "for language in LANGUAGES:\n",
    "    vocab_transform[language] = torchtext.vocab.build_vocab_from_iterator(\n",
    "        yield_tokens(Multi30k(split=\"train\", language_pair=LANGUAGES), language),\n",
    "        min_freq=MINIMUM_FREQUENCY,\n",
    "        specials=SPECIAL_SYMBOLS,\n",
    "        special_first=True,\n",
    "    )\n",
    "    # Set UNK_IDX as the default index. This is returned when the token is not found.\n",
    "    vocab_transform[language].set_default_index(SPECIAL_SYMBOL_INDICES[UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some helper functions for messing with natural language datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    \"\"\"\n",
    "    Compose a bunch of transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids: list[int]):\n",
    "    \"\"\"\n",
    "    Add BOS/EOS and create tensor for input sequence indices\n",
    "    \"\"\"\n",
    "    return torch.cat(\n",
    "        (\n",
    "            torch.tensor([SPECIAL_SYMBOL_INDICES[BOS]]),\n",
    "            torch.tensor(token_ids),\n",
    "            torch.tensor([SPECIAL_SYMBOL_INDICES[EOS]]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Source and target language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {\n",
    "    language: sequential_transforms(token_transform[language], vocab_transform[language], tensor_transform)\n",
    "    for language in LANGUAGES\n",
    "}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate data samples into batch tensors\n",
    "    \"\"\"\n",
    "    sourcebatch, target_batch = [], []\n",
    "    for sourcesample, target_sample in batch:\n",
    "        sourcebatch.append(text_transform[SOURCE_LANGUAGE](sourcesample.rstrip(\"\\n\")))\n",
    "        target_batch.append(text_transform[TARGET_LANGUAGE](target_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    sourcebatch = nn.utils.rnn.pad_sequence(sourcebatch, padding_value=SPECIAL_SYMBOL_INDICES[PAD])\n",
    "    target_batch = nn.utils.rnn.pad_sequence(target_batch, padding_value=SPECIAL_SYMBOL_INDICES[PAD])\n",
    "    return sourcebatch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create triangular masks and padding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, target):\n",
    "    sourceseq_len = src.shape[0]\n",
    "    target_seq_len = target.shape[0]\n",
    "\n",
    "    target_mask = Transformer.generate_square_subsequent_mask(target_seq_len, DEVICE)\n",
    "    source_mask = torch.zeros((sourceseq_len, sourceseq_len), device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    source_padding_mask = torch.zeros_like(src, dtype=torch.float32)\n",
    "    target_padding_mask = torch.zeros_like(target, dtype=torch.float32)\n",
    "    source_padding_mask[src == SPECIAL_SYMBOL_INDICES[PAD]] = -torch.inf\n",
    "    target_padding_mask[target == SPECIAL_SYMBOL_INDICES[PAD]] = -torch.inf\n",
    "    source_padding_mask = source_padding_mask.transpose(0, 1)\n",
    "    target_padding_mask = target_padding_mask.transpose(0, 1)\n",
    "\n",
    "    return source_mask, target_mask, source_padding_mask, target_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define the hyperparameters for our Transformer. If you have difficulty running this notebook, change these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VOCAB_SIZE = len(vocab_transform[SOURCE_LANGUAGE])\n",
    "TARGET_VOCAB_SIZE = len(vocab_transform[TARGET_LANGUAGE])\n",
    "EMBED_SIZE = 512\n",
    "NUM_HEADS = 8\n",
    "FEEDFORWARD_HIDDEN_SIZE = 512\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make the train and validation data loaders. I happened to know the number of training and validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    Multi30k(split=\"train\", language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    Multi30k(split=\"valid\", language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# Used for a working progress bar.\n",
    "N_TRAIN = 29001\n",
    "N_VALIDATION = 1050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model, loss function, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMBED_SIZE,\n",
    "    NUM_HEADS,\n",
    "    SOURCE_VOCAB_SIZE,\n",
    "    TARGET_VOCAB_SIZE,\n",
    "    FEEDFORWARD_HIDDEN_SIZE,\n",
    ")\n",
    "for parameter in model.parameters():\n",
    "    if parameter.dim() > 1:\n",
    "        nn.init.xavier_uniform_(parameter)\n",
    "model = model.to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=SPECIAL_SYMBOL_INDICES[PAD])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for source, target in (\n",
    "        progress_bar := tqdm.tqdm(\n",
    "            train_dataloader, total=(N_TRAIN + BATCH_SIZE - 1) // BATCH_SIZE, desc=f\"Epoch {epoch}\"\n",
    "        )\n",
    "    ):\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        target_input = target[:-1, :]\n",
    "        source_mask, target_mask, source_padding_mask, target_padding_mask = create_masks(source, target_input)\n",
    "        logits = model(\n",
    "            source,\n",
    "            target_input,\n",
    "            source_mask,\n",
    "            target_mask,\n",
    "            source_padding_mask,\n",
    "            target_padding_mask,\n",
    "            source_padding_mask,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target_out = target[1:, :]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total += target.shape[1]\n",
    "\n",
    "        progress_bar.set_postfix_str(f\"train loss: {train_loss / total:.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for source, target in (\n",
    "        progress_bar := tqdm.tqdm(\n",
    "            validation_dataloader, total=(N_VALIDATION + BATCH_SIZE - 1) // BATCH_SIZE, desc=f\"Validation\"\n",
    "        )\n",
    "    ):\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        target_input = target[:-1, :]\n",
    "        source_mask, target_mask, source_padding_mask, target_padding_mask = create_masks(source, target_input)\n",
    "        logits = model(\n",
    "            source,\n",
    "            target_input,\n",
    "            source_mask,\n",
    "            target_mask,\n",
    "            source_padding_mask,\n",
    "            target_padding_mask,\n",
    "            source_padding_mask,\n",
    "        )\n",
    "        target_out = target[1:, :]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_out.reshape(-1))\n",
    "        validation_loss += loss.item()\n",
    "        total += target.shape[1]\n",
    "\n",
    "        progress_bar.set_postfix_str(f\"validation loss: {validation_loss / total:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following `translate` function to try translating a German sentence to English. Note that we trained on **descriptions of images**, so the model will do better on sentences like that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, max_length, start_symbol):\n",
    "    source = source.to(DEVICE)\n",
    "    memory = model.encode(source, source_mask)\n",
    "    predicted_tokens = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for _ in range(max_length - 1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        target_mask = Transformer.generate_square_subsequent_mask(predicted_tokens.size(0)).to(DEVICE)\n",
    "        out = model.decode(predicted_tokens, memory, target_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        predicted_tokens = torch.cat([predicted_tokens, torch.ones(1, 1).type_as(source.data).fill_(next_word)], dim=0)\n",
    "        if next_word == SPECIAL_SYMBOL_INDICES[EOS]:\n",
    "            break\n",
    "    return predicted_tokens\n",
    "\n",
    "def translate(model: torch.nn.Module, sourcesentence: str):\n",
    "    model.eval()\n",
    "    source = text_transform[SOURCE_LANGUAGE](sourcesentence).view(-1, 1)\n",
    "    num_tokens = source.shape[0]\n",
    "    source_mask = None\n",
    "    target_tokens = greedy_decode(\n",
    "        model, source, source_mask, max_length=num_tokens + 5, start_symbol=SPECIAL_SYMBOL_INDICES[BOS]\n",
    "    ).flatten()\n",
    "    return (\n",
    "        \" \".join(vocab_transform[TARGET_LANGUAGE].lookup_tokens(list(target_tokens.cpu().numpy())))\n",
    "        .replace(BOS, \"\")\n",
    "        .replace(EOS, \"\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, \"Eine Gruppe von Mnnern ldt Baumwolle auf einen Lastwagen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target translation was \"A group of men are loading cotton onto a truck.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
