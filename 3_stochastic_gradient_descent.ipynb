{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "To more conveniently use arrays in Python, we will use the PyTorch numerical computing framework. To install it, go to https://pytorch.org/get-started and follow the instructions for your system. Once we have PyTorch installed, we import `torch` to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following if necessary.\n",
    "* `matplotlib` for plotting. Install it using `pip install matplotlib`. Or `conda install matplotlib` if you prefer.\n",
    "* `tqdm` provides a nice progress bar. `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing with Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important data structure in PyTorch is the `Tensor`, which is an array of numbers, all of the same type, usually some kind of floating-point number. In the machine learning community, such arrays are often called *tensors*, even when they aren't being used to represent mathematical [tensors](https://en.wikipedia.org/wiki/Tensor), which are linear maps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create `Tensor`s by passing in Python `tuple`s or `list`s to the `tensor` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor((1, 2, 3, 4))\n",
    "b = torch.tensor((( 5,  6,  7,  8),\n",
    "                  (-5, -6, -7, -8)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `Tensor`s have `shape`s. Here are their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add, subtract, and negate `Tensor`s using + and -."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + a, a - a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is applied automatically where it's unambiguous. (Read the 2_math_review notebook if you are unsure what broadcasting does.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication can be done using the `@` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b @ a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise multiplication is automatically broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select elements or slices of `Tensor`s using`[index]`. This `index` can be a variety of things. I'll show you a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (0, 0)\n",
    "b[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:, :2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See PyTorch's [Tensor indexing API](https://pytorch.org/cppdocs/notes/tensor_indexing.html) for more examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "Gradient descent is an optimization method that adjusts the parameters of a differentiable function that captures some sort of \"loss,\" or the error of an estimation, in order to optimize it. [Here](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) is a nice YouTube playlist about it by 3Blue1Brown. I'll demonstrate it through a regression task. Regression is predicting a real number or real array.\n",
    "\n",
    "Suppose there were some function $f$ that maps real numbers to real numbers. That is, $f: \\mathbb{R} \\to \\mathbb{R}$. Let's make an $f$ that is actually $e^x +$ some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: e^x + some random noise\n",
    "    \"\"\"\n",
    "    return torch.exp(x) + 0.1 * torch.randn(len(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's pretend we don't know what $f$ is. We only see some examples of $x$ mapped to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y = f(x)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a trainable function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attempt to approximate $f$ using a differentiable parameterized function $\\hat{f}$ with much fewer parameters than the number of examples.\n",
    "\n",
    "> Worth discussing with your friends: Why do we want much fewer parameters than the number of examples?\n",
    "\n",
    "We will train parameters $\\theta$ for $\\hat{f}$ to minimize some differentiable loss function $L$ between $\\hat{f}(x)$ and $y$ for all observations $(x, y)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a function such that we can repeatedly adjust $\\theta$ against the gradient of $L$ with respect to $\\theta$, at some learning rate $\\lambda$. That is,\n",
    "\n",
    "Do $\\quad \\theta \\gets \\theta - \\lambda \\nabla_\\theta L(\\hat{f}(x), y) \\quad$ for all $(x, y)$ repeatedly until it's good enough.\n",
    "\n",
    "Typical artificial neural networks (ANNs) are functions that allow us to do that. Let's make one kind of ANN called a multilayer perceptron (MLP), also called a feedforward neural network, also called a fully connected neural network.\n",
    "\n",
    "For now, let $\\hat{f}$ be an MLP that takes a **single scalar** input $x$ and outputs a number $\\hat{y}$. We will make it handle a batch of inputs later.\n",
    "\n",
    "Here is what will happen inside.\n",
    "1. We apply a linear transformation.\n",
    "2. We apply a nonlinear transformation.\n",
    "3. We apply a linear transformation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **first linear transformation**, we will multiply $x$ by a vector of 4 weights $\\mathbf{W_1}$, and then add a vector of 4 biases $\\mathbf{B_1}$ to that. I've given these parameters the subscript $\\mathbf{_1}$ because they belong to the 1st linear transformation. Both $\\mathbf{W_1}$ and $\\mathbf{B_1}$ will be shaped $4 \\times 1$. Let's call the result $\\mathbf{Z}$.\n",
    "\n",
    "$\\mathbf{Z} = \\mathbf{W_1} x + \\mathbf{B_1}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply a **nonlinear transformation** to $\\mathbf{z}$. There are many we can choose from, but we usually use the rectified linear unit (ReLU). It's just a function that returns the max of 0 and the input.\n",
    "\n",
    "$\\mathrm{ReLU}(z) = \\max(0, z)$\n",
    "\n",
    "Let's apply it element-wise to $\\mathbf{Z}$ and call the result $\\mathbf{A}$ for \"activation.\"\n",
    "\n",
    "$\\mathbf{A} = \\mathrm{ReLU}(\\mathbf{Z})$\n",
    "\n",
    "Because machine learning (ML) people tend to use brain analogies, 4 elements in $\\mathbf{A}$ are often called *neurons*, i.e. there are 4 neurons in this layer. They're also called *hidden units*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the **second linear transformation**. $\\mathbf{A}$ is a $4 \\times 1$ matrix. Since we want a single-element output, we must multiply it by a $1 \\times 4$ matrix $\\mathbf{W_2}$ and then add a $1 \\times 1$ matrix $\\mathbf{B_2}$.\n",
    "\n",
    "$\\hat{y} = \\mathbf{W_2 A} + \\mathbf{B_2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching is an essential component in modern machine learning because it allows us to train functions quickly by leveraging parallel multiprocessors, i.e. graphics processing units (GPUs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want $\\hat{f}$ to take a batch of inputs $\\mathbf{X}$. We need to consider what shape $\\mathbf{X}$ should be. The convention is to make the first dimension the batch dimension, followed by the shape of a single $x$. Since our $x$ is just a single number, our $\\mathbf{X}$ is shaped $(\\mathrm{batch\\ size} \\times 1)$. We also want $\\hat{\\mathbf{Y}}$, the batch of outputs from $\\hat{f}$, to have the batch dimension first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we transpose the $\\mathbf{W}$ s, so that $\\mathbf{W_1}$ is shaped $1 \\times 4$ and $\\mathbf{W_2}$ is shaped $4 \\times 1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we change $\\hat{f}$ thus:\n",
    "> Instead of multiplying $\\mathbf{W_1}x$, we do $\\mathbf{XW_1}$.\n",
    "\n",
    "> Instead of multiplying $\\mathbf{W_2 A}$, we do $\\mathbf{AW_2}$.\n",
    "\n",
    "Then $\\hat{\\mathbf{Y}}$ will be shaped $(\\mathrm{batch\\ size} \\times 1)$. You can ponder about it and work out a few examples until you're convinced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's write it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure $\\mathbf{X}$ and $\\mathbf{Y}$ are the shape we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now they're 1-dimensional arrays. We want to make them 2-dimensional by adding a second dimension of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:, None]\n",
    "y = y[:, None]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write $\\hat{f}$.\n",
    "\n",
    "First, write a linear function that returns $\\mathbf{Z}$ from $x$. It should be about one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(x: torch.Tensor, w1: torch.Tensor, b1: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: z\n",
    "\n",
    "    I usually write better docstrings, but that would just give you the solution.\n",
    "    \"\"\"\n",
    "    return ...# TODO "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'cmV0dXJuIHggQCB3MSArIGIx').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the nonlinear function that returns $\\mathbf{A}$. Take $\\mathbf{Z}$ as input. One line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: a\n",
    "    \"\"\"\n",
    "    return ...# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'cmV0dXJuIHRvcmNoLnJlbHUoeik=').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the second linear function that returns $\\hat{y}$. Take $\\mathbf{A}$ as input. One line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear2(a: torch.Tensor, w2: torch.Tensor, b2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: y hat\n",
    "    \"\"\"\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'cmV0dXJuIGEgQCB3MiArIGIy').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write $\\hat{f}$ as a composition of the first linear, the nonlinear, and the second linear functions. Instead of just returning $\\hat{y}$, we're going to also return $\\mathbf{Z}$ and $\\mathbf{A}$ so we can use them to calculate gradients later. This should take about 3 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_hat(x: torch.Tensor, \n",
    "          w1: torch.Tensor, \n",
    "          b1: torch.Tensor,\n",
    "          w2: torch.Tensor, \n",
    "          b2: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    :returns: a tuple (z, a, y hat)\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'eiA9IGxpbmVhcjEoeCwgdzEsIGIxKQphID0gbm9ubGluZWFyKHopCnJldHVybiB6LCBhLCBsaW5lYXIyKGEsIHcyLCBiMik=').decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function, we can use the mean squared error $L = \\text{MSE}(\\hat{\\mathbf{Y}}, \\mathbf{Y}) = \\dfrac{\\sum_{i=1}^{|\\mathbf{Y}|}(\\hat{\\mathbf{Y}_i} - \\mathbf{Y}_i)^2}{|\\mathbf{Y}|}$. I'll write this one for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param y_hat: predictions\n",
    "    :param y: labels\n",
    "    :returns: the mean squared error between y_hat and y.\n",
    "    \"\"\"\n",
    "    return ((y_hat - y) ** 2).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap,\n",
    "> We wanted a differentiable parameterized function that could approximate $f$. We now have this as $\\hat{f}$.\n",
    "\n",
    "> We also wanted a differentiable function that could capture the error between the output of $\\hat{f}$ and the output of $f$. We now have this as $L$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how to adjust the parameters, we must find the gradient of the loss function with respect to each parameter. Once we find that, we just nudge the parameters in a direction that decreases the loss. Because we can step backwards through the operations using the chain rule, this is called *backpropagation*, while applying $\\hat{f}$ is called *forward propagation*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the gradients from the back. In order to go from $L$ to all the parameters, we have to go through $\\hat{y}$. What's $\\nabla_{\\hat{y}} L$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss_wrt_y_hat(y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param y_hat: predictions\n",
    "    :param y: labels\n",
    "    :returns: the derivative of the loss with respect to the predictions.\n",
    "    \"\"\"\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'cmV0dXJuIDIgKiAoeV9oYXQgLSB5KQ==').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have the $\\mathbf{W_2}$ and $\\mathbf{B_2}$ in our sights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_y_hat_wrt_w2(a: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param a: output of the nonlinear function\n",
    "    :returns: the derivative of the second linear function with respect to w2.\n",
    "    \"\"\"\n",
    "    return a.sum(0)\n",
    "\n",
    "def d_y_hat_wrt_b2() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: the derivative of the second linear function with respect to b2.\n",
    "    \"\"\"\n",
    "    return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although $\\mathbf{A}$ is not a parameter, we have to go through it in order to get to the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_y_hat_wrt_a(w2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: the derivative of the second linear function with respect to a.\n",
    "    \"\"\"\n",
    "    return w2.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}$ was the output of the nonlinear function, i.e. ReLU. Although in pure math, ReLU is not differentiable, we treat its derivative as 1 if its input is greater than 0 and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_a_wrt_z(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param z: the output of the first linear function\n",
    "    :returns: the derivative of ReLU with respect to z\n",
    "    \"\"\"\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'cmV0dXJuIHogPiAw').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $\\mathbf{Z}$, we can see $\\mathbf{W_1}$ and $\\mathbf{B_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_z_wrt_w1(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param x: input examples\n",
    "    :returns: the derivative of the first linear function with respect to w1\n",
    "    \"\"\"\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_z_wrt_b1() -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param x: input examples\n",
    "    :returns: the derivative of the first linear function with respect to b1\n",
    "    \"\"\"\n",
    "    return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, we can repeatedly update\n",
    "* $\\mathbf{W_2} \\gets \\mathbf{W_2} - \\lambda \\nabla_{\\mathbf{W_2}} L$\n",
    "* $\\mathbf{B_2} \\gets \\mathbf{B_2} - \\lambda \\nabla_{\\mathbf{B_2}} L$\n",
    "* $\\mathbf{W_1} \\gets \\mathbf{W_1} - \\lambda \\nabla_{\\mathbf{W_1}} L$\n",
    "* $\\mathbf{B_1} \\gets \\mathbf{B_1} - \\lambda \\nabla_{\\mathbf{B_1}} L$\n",
    "\n",
    "\n",
    "until we approach the capability of $\\hat{f}$ to approximate $f$. This update size is controlled by a usually small learning rate $\\lambda$ that gets smaller over time. Here, we'll just keep it constant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function, fill in these gradients.\n",
    "* $\\nabla_{\\mathbf{W_2}} L$\n",
    "* $\\nabla_{\\mathbf{B_2}} L$\n",
    "* $\\nabla_{\\mathbf{W_1}} L$\n",
    "* $\\nabla_{\\mathbf{B_1}} L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "def update_parameters(z: torch.Tensor,\n",
    "                      a: torch.Tensor,\n",
    "                      y_hat: torch.Tensor,\n",
    "                      y: torch.Tensor,\n",
    "                      w1: torch.Tensor,\n",
    "                      b1: torch.Tensor,\n",
    "                      w2: torch.Tensor,\n",
    "                      b2: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Update the parameters by nudging them against the gradient of the loss with respect to the parameters.\n",
    "    \"\"\"\n",
    "    d_loss_wrt_w2 = ... # TODO\n",
    "    d_loss_wrt_b2 = ... # TODO\n",
    "    d_loss_wrt_w1 = ... # TODO\n",
    "    d_loss_wrt_b1 = ... # TODO\n",
    "\n",
    "    w2 -= LEARNING_RATE * d_loss_wrt_w2[:, None]\n",
    "    b2 -= LEARNING_RATE * d_loss_wrt_b2\n",
    "    w1 -= LEARNING_RATE * d_loss_wrt_w1[None, :]\n",
    "    b1 -= LEARNING_RATE * d_loss_wrt_b1\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'VXNlIHRoZSBjaGFpbiBydWxlLg==').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base64.b64decode(b'VGhlIGRlcml2YXRpdmUgb2YgYSBtZWFuIGlzIHRoZSBtZWFuIGRlcml2YXRpdmUu').decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'CmRfbG9zc193cnRfdzIgPSAoZF9sb3NzX3dydF95X2hhdCh5X2hhdCwgeSkgKiBkX3lfaGF0X3dydF93MihhKSkubWVhbigwKQpkX2xvc3Nfd3J0X2IyID0gKGRfbG9zc193cnRfeV9oYXQoeV9oYXQsIHkpICogZF95X2hhdF93cnRfYjIoKSkubWVhbigwKQpkX2xvc3Nfd3J0X3cxID0gKGRfbG9zc193cnRfeV9oYXQoeV9oYXQsIHkpICogZF95X2hhdF93cnRfYSh3MikgKiBkX2Ffd3J0X3ooeikgKiBkX3pfd3J0X3cxKHgpKS5tZWFuKDApCmRfbG9zc193cnRfYjEgPSAoZF9sb3NzX3dydF95X2hhdCh5X2hhdCwgeSkgKiBkX3lfaGF0X3dydF9hKHcyKSAqIGRfYV93cnRfeih6KSAqIGRfel93cnRfYjEoKSkubWVhbigwKQo=').decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the gradient descent function. We will first initialize the parameters: the $\\mathbf{W}$ s randomly and the $\\mathbf{B}$ s to $\\mathbf{0}$. Then start a loop for a number of iterations, called *epochs*, which is often decided beforehand, applying the updates to these parameters after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50000\n",
    "N_HIDDEN_UNITS = 4\n",
    "\n",
    "w1 = torch.randn(1, N_HIDDEN_UNITS)\n",
    "b1 = torch.zeros(N_HIDDEN_UNITS)\n",
    "w2 = torch.randn(N_HIDDEN_UNITS, 1)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "def gradient_descent(w1: torch.Tensor, \n",
    "                     b1: torch.Tensor, \n",
    "                     w2: torch.Tensor, \n",
    "                     b2: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Optimize parameters through gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    for _ in (progress_bar := tqdm.tqdm(range(N_EPOCHS), desc=\"training\")):\n",
    "        z, a, y_hat = f_hat(x, w1, b1, w2, b2)\n",
    "        w1, b1, w2, b2 = update_parameters(z, a, y_hat, y, w1, b1, w2, b2)\n",
    "        progress_bar.set_postfix_str(f\"loss: {loss(y_hat, y)}\")\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just random parameters, $\\hat{f}$ shouldn't be good at approximating $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(x.cpu().numpy(), y)\n",
    "plt.plot(x.cpu().numpy(), \n",
    "         f_hat(x, w1, b1, w2, b2)[2].cpu().numpy(), \n",
    "         color=\"red\", \n",
    "         label=\"random parameters\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = gradient_descent(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(x.cpu().numpy(), y)\n",
    "plt.plot(x.cpu().numpy(), \n",
    "         f_hat(x, w1, b1, w2, b2)[2].cpu().numpy(), \n",
    "         color=\"red\", \n",
    "         label=\"trained parameters\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) is just gradient descent, but instead of updating parameters after looking at all examples, you update them after looking at a smaller batch of them, called a minibatch, or just a batch. This may be necessary when all training data can't fit into your memory. An epoch is counted when you look at all examples once.\n",
    "\n",
    "Suppose we use minibatches of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training with SGD, you should shuffle your training data, just in case there's some order to them that would cause weird training behavior. The PyTorch way to do this is to create a PyTorch `Dataset` out of the data, and load it into your training loop with a `DataLoader`, which takes care of shuffling it every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(data.TensorDataset(x, y), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "        w1: torch.Tensor, \n",
    "        b1: torch.Tensor, \n",
    "        w2: torch.Tensor, \n",
    "        b2: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Optimize parameters through stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    for _ in (progress_bar := tqdm.tqdm(range(N_EPOCHS), desc=\"training\")):\n",
    "        for x, y in dataloader:\n",
    "            z, a, y_hat = f_hat(x, w1, b1, w2, b2)\n",
    "            w1, b1, w2, b2 = update_parameters(z, a, y_hat, y, w1, b1, w2, b2)\n",
    "            progress_bar.set_postfix_str(f\"minibatch loss: {loss(y_hat, y)}\")\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reinitialize the parameters and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(1, N_HIDDEN_UNITS)\n",
    "b1 = torch.zeros(N_HIDDEN_UNITS)\n",
    "w2 = torch.randn(N_HIDDEN_UNITS, 1)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "w1, b1, w2, b2 = stochastic_gradient_descent(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(x.cpu().numpy(), y)\n",
    "plt.plot(x.cpu().numpy(), \n",
    "         f_hat(x, w1, b1, w2, b2)[2].cpu().numpy(), \n",
    "         color=\"red\", \n",
    "         label=\"trained parameters\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zugzwang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
